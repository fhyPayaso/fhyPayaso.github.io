#吴恩达机器学习(1-7)


## 一、 监督学习与无监督学习

+ 监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。(关键在于数据集的分类标签或数值是已知的)在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。比如你朋友那个新房子的价格。用术语来讲，这叫做回归问题。我们试着推测出一个连续值的结果，即房子的价格。

	+ 回归问题: 预测连续值的问题, 例如判断房子的价格
	
	+ 分类问题 ： 预测离散值的问题, 例如通过肿瘤大小、病人年龄等不同维度判断肿瘤是否为良性。

+ 无监督学习指数据集本身并无标注信息或分类信息 , 例如给出一组数据需要算法自动根据特征信息进行分类, 即无监督学习中的聚类算法 。

## 二、 线性回归与梯度下降

### 1. 线性回归

![](http://img.fhypayaso.cn/QQ20200820-143637%402x.png)

**单变量的线性回归函数**$h_\theta(x) $ ：

$$ h_\theta(x) = \theta_0 + \theta_1x$$

以房价问题为例,我们需要通过现有的数据集拟合一条直线，来对一般数据进行预测,其中

+ $x$ :房子的平米数
+ $h_\theta(x)$ :房子的售价
+ $\theta_1$ :拟合直线的斜率
+ $\theta_0$ : 拟合直线与纵轴焦点



### 2. 代价函数

而对于不同的$\theta_0$和$\theta_1$ 所拟合的直线与真实数据都会存在一定误差, 可以用**代价函数**来表示,


$$ J(\theta_0 , \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} {(h_\theta (x^{(i)})  - y^{(i)})}^2$$

即求回归直线与真实数据集的误差平方和并求平均, 所以目标变成求$minimize J(\theta_0 , \theta_1)$所对应的$\theta_0$和$\theta_1$

对于只有一个参数的回归直线, 其代价函数图像如下所示, 可以看到代价函数为一个最小值为0的二次曲线 

![](http://img.fhypayaso.cn/QQ20200820-150438@2x.png)

而对于两个参数的回归直线, 代价函数为

![](http://img.fhypayaso.cn/QQ20200820-150717@2x.png)
	
### 3. 梯度下降

梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数$J(\theta_0 , \theta_1)$的最小值。

梯度下降背后的思想是：开始时我们随机选择一个$\theta$参数的组合，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合，可能会找到不同的局部最小值

![](http://img.fhypayaso.cn/QQ20200820-151420@2x.png)

例如一个复杂的代价函数的三维表示, 在山顶位置任选一点, 并且每次都向着下降幅度最大的方向前进, 但是最终能够下降到哪个最低点, 与初始位置的选择有关, 批量梯度下降**(batch gradient descent)**的算法公式为

![](http://img.fhypayaso.cn/QQ20200820-152208@2x.png)


其中$\alpha$是学习率**（learning rate）**，它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的偏导数。

其中有一个很关键的步骤在于，所有的参数都需要**同时**进行变化, 而不是先更新$\theta_0$再更新$\theta_1$,  因为这样的话先更新的参数会影响后面参数更新的结果。

![](http://img.fhypayaso.cn/QQ20200820-152659@2x.png)

我们还是采用一个参数的情况来理解梯度下降公式,  对于代价函数图像上的某一个点, 如果当前切线的斜率为正, 即导数为正数，那么在应用公式的时 ,  减去正数即向负向移动, 损失函数下降。而如果斜率为负, 相当于减去负数向正向移动, 损失函数依然下降。

其中学习率$\alpha$的选择不能过大或者过小, 若$\alpha$过小, 会导致每次移动下降幅度过小, 下降速度慢。如果$\alpha$太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果太大，它会导致无法收敛，甚至发散。

同时$\alpha$在下降过程中也不需要额外变化。在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，所以当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度。所以实际上没有必要再另外减小$\alpha$。
	

### 4. 线性回归中的梯度下降

现在我们回顾一下两个算法:

+ 线性回归算法: 

	+ 回归方程 : $ h_\theta(x) = \theta_0 + \theta_1x$
	
	+ 损失函数: $ J(\theta_0 , \theta_1) = \frac{1}{2m} \sum_{i=0}^{m} {(h_\theta (x^{(i)})  - y^{(i)})}^2$

+ 梯度下降算法

	**Repeat until convergence {**
	
	$\theta_j = \theta_j - \alpha \frac{\vartheta}{\vartheta\theta_j} J(\theta_0 , \theta_1) $
	
	$ for( j = 1  ， j = 0) $
	
	**}**
	
将损失函数带入到梯度下降算法 , 即: 

$$\frac{\vartheta}{\vartheta\theta_j} J(\theta_0 , \theta_1)  = \frac{\vartheta}{\vartheta\theta_j} \frac{1}{2m} \sum_{i=1}^{m} {(h_\theta (x^{(i)})  - y^{(i)})}^2$$

+ **j=0**

$$\frac{\vartheta}{\vartheta\theta_0} J(\theta_0 , \theta_1)  = \frac{1}{m} \sum_{i=1}^{m} (h_\theta (x^{(i)})  - y^{(i)})$$

+ **j=1**

$$\frac{\vartheta}{\vartheta\theta_1} J(\theta_0 , \theta_1)  = \frac{1}{m} \sum_{i=1}^{m} ((h_\theta (x^{(i)})  - y^{(i)}) \cdot  x^{(i)})$$

则梯度下降算法可以改写为
	
**Repeat until convergence {**
	
$\theta_0 = \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta (x^{(i)})  - y^{(i)}) $
	
$\theta_1 = \theta_1 - \alpha \frac{1}{m} \sum_{i=1}^{m} ((h_\theta (x^{(i)})  - y^{(i)}) \cdot  x^{(i)})$
	
**}**

该算法有时也称为批量梯度下降，指的是在梯度下降的每一步中，都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都需要对所有m个训练样本求和。因此，批量梯度下降法这个名字说明了我们需要考虑所有这一"批"训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种"批量"型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。


## 三、 多元线性回归

### 1. 多维特征

上一部分举了房子面积与售价关系的例子, 但实际上 ,影响房子售价的因素可能有很多，如房间数，楼层数等等, 

![](http://www.ai-start.com/ml2014/images/591785837c95bca369021efa14a8bb1c.png)

所以多变量的线性回归方程$h_\theta(x)$可以定义为: 

$$ h_\theta(x) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + ... + + \theta_nx_n$$

+ $n$代表特征的数量, 例如在上表中 $n=4$

+ $x^{(i)}$代表第$i$个训练实例，是特征矩阵中的第$i$行，是一个**向量（vector）** , 例如在上表中,

$$x^{(2)} = \left[ \begin{matrix} 1416 \\ 3 \\ 2 \\ 40 \end{matrix} \right]$$

+ 而$x_j^{(i)}$代表第 $i$ 行的第 $j$ 个特征值, 例如$x_3^{(2)} = 2$ 

+ $i$ 的范围为 $[1 , m]$  , 而 $j$ 的范围则为$[0 , n]$,   因为为了匹配$\theta_0$ ， 一般会添加一个 $x_0 = 1$ 与之匹配

+ 所以特征矩阵的大小为$m * (n + 1)$ ， 所以$h_\theta(x)$可以简化为 $h_\theta(x) = \theta^TX$

### 2. 多维梯度下降

同样的可以将多维特征方程带入到线性回归中的梯度下降函数中, 得到多维梯度下降的函数:

**Repeat until convergence {**
		
$\theta_j = \theta_j - \alpha \frac{1}{m} \sum_{i=1}^{m} ((h_\theta (x^{(i)})  - y^{(i)}) \cdot  x_j^{(i)})$ , $j$ from $0$ to $n$
	
**}**


### 3. 特征缩放

在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。

以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。如图：

![](http://www.ai-start.com/ml2014/images/b8167ff0926046e112acf789dba98057.png)

特征缩放的处理方式如下, 其中$\mu_n$为当前特征的平均值,$s_n$为当前特征的标准差,

$$ x_n =  \frac{x_n - \mu_n}{s_n}$$

### 4. 学习率的选择

![](http://www.ai-start.com/ml2014/images/cd4e3df45c34f6a8e2bb7cd3a2849e6c.jpg)

梯度下降算法的每次迭代受到学习率的影响，如果学习率过小，则达到收敛所需的迭代次数会非常高；如果学习率过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。

通常可以考虑尝试些学习率：

$$ \alpha = 0.01 , 0.03 , 0.1 , 0.3 , 1 , 3 , 10  $$

### 5. 多项式回归

线性回归并不适用于所有数据*(例如在房价预测问题中特征数据为长宽而不是面积)*，有时我们需要曲线来适应我们的数据，比如一个二次方模型：$ h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2^2$ 或者三次方模型： $ h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2^2 + \theta_3x_3^3$

通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外可以令：$x_2 = x_2^2 , x_3 = x_3^3$，从而将模型转化为线性回归模型。

**注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。**

### 6. 正规方程

上述的内容都是通过梯度下降的方法来求得 $\theta$ 使 $J(\theta)$ 取最小值, 还有一种方法可以通过正规方程直接算出最优解的$\theta$, 正规方程表达式如下, 其中特征矩阵$X$包含了$x_0 = 1$,  

$$\theta = (X^TX)^{-1} X^T y $$

梯度下降与正规方程的比较：

| 梯度下降 | 正规方程 |
| --- | --- |
| 需要选择学习率 | 不需要 |
| 需要特征归一化 | 不需要 |
| 需要多次迭代 | 一次运算得出 |
| 当特征数量大时也能较好适用 | 需要计算 $(X^TX)^{-1}$ 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为$O(n^3)$，通常来说当小于10000 时还是可以接受的 |
| 适用于各种类型的模型 | 只适用于线性模型，不适合逻辑回归模型等其他模型 |


## 四、 逻辑回归

在分类问题中，你要预测的变量  是离散的值，我们将学习一种叫做**逻辑回归 (Logistic Regression)** 的算法，这是目前最流行使用最广泛的一种学习算法。

在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；之前我们也谈到了肿瘤分类问题的例子，区别一个肿瘤是恶性的还是良性的

如果我们要用线性回归算法来解决一个分类问题，对于二分类， 取值为 0 或者1，但假设函数的输出值可能远大于 1或远小于0，即使所有训练样本的标签都等于 0 或 1 , 这并不是我们所期望的。所以我们在接下来的要研究的算法就叫做逻辑回归算法，这个算法的性质是：它的输出值永远在0到 1 之间。

### 1. 假设函数

线性回归算法处理二分类问题, 会出现预测值远大于 1或远小于0的情况,  , 这时需要将输出值控制在0到1之间,  这样大于0.5的预测值可被分类为1 , 小于0.5的预测值可被分类为0。 这里引入逻辑回归的模型 : $h_\theta(x) = g(\theta^T X)$

其中g代表**逻辑函数（logistic function)**,  最常用的逻辑函数叫做S形函数即**Sigmoid function**, 公式为: 

$$ g(z) = \frac{1}{1 + e ^{-z}} $$

该函数的图像为:  ![](http://www.ai-start.com/ml2014/images/1073efb17b0d053b4f9218d4393246cc.jpg)

对$h_\theta(x)$重新理解，对于给定的输入变量，根据选择的参数计算输出变量=1的可能性**（estimated probablity）**即$h_\theta(x) = P(y = 1 | x ; \theta)$ 。 如果对于给定的x，通过已经确定的参数计算得出$h_\theta(x) = 0.7$ ，则表示有70%的几率为正向类，相应地为负向类的几率为1-0.7=0.3。

### 2. 决策边界(decision boundary)

在逻辑回归中 , 我们一般预测

$h_\theta(x) >= 0.5$ ， 预测 $ y = 1 $

$h_\theta(x) < 0.5$ ， 预测 $ y = 0 $

且 $h_\theta(x) = g(\theta^T X)$ , 同时我们又知道Sigmoid函数的图像, 当$z = 0$ 时 , $g(z) = 0.5$， 且函数递增，

所以可以将问题转化为当 $\theta^T X >= 0 $ 时, 预测值为1 , 反之预测值为0 ,  由此可以用来计算决策边界


![](http://img.fhypayaso.cn/QQ20200822-191408@2x.png)
	
### 3. 代价函数与梯度下降

在逻辑回归中， 我们依然需要建立一个代价函数来表示当前参数$\theta$的优劣情况, 并对此求最优解, 但若直接将逻辑回归的假设函数$h_\theta(x) $, 带入到之前学习过的代价函数中， 会发现得到的结果如下图左侧所示, 得到的是一个**非凸函数(non-convexfunction)**， 这意味着代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。

![](http://www.ai-start.com/ml2014/images/8b94e47b7630ac2b0bcb10d204513810.jpg)

这里我们重新定义逻辑回归的代价函数: 

$$ J(\theta) = \frac{1}{m} \sum_{i=1}^{m} Cost(h_\theta (x^{(i)}) , y^{(i)})$$

其中: 

$$ Cost(h_\theta (x) , y) = \begin{cases} -log(h_\theta (x))  ,  y = 1 \\ -log( 1- h_\theta (x))  , y = 0 \end{cases} $$

$h_\theta(x) $与$Cost(h_\theta (x) , y)$的关系如下

![](http://www.ai-start.com/ml2014/images/ffa56adcc217800d71afdc3e0df88378.jpg)

+ 当y = 1 时
	+ 预测值为1，与数据集相同 ,损失函数为0
	+ 预测值为0，与数据集相反 ，损失函数无穷大,   相当于对错误预测给予了一个非常大的惩罚
	+ y = 0时 同理

可以将$Cost$函数进行合并书写: 

$$ Cost(h_\theta (x) , y) = -y * log(h_\theta (x)) - (1-y) * log( 1- h_\theta (x))$$

带入损失函数$J(\theta)$:

$$ J(\theta) = -\frac{1}{m}  \sum_{i=1}^{m} [ y^{(i)} * log(h_\theta (x^{(i)})) + (1-y^{(i)}) * log( 1- h_\theta (x^{(i)}) ]$$

得到代价函数后, 便可以使用梯度下降算法, 来求得使代价函数最小的参数$\theta$

**Repeat until convergence {**
		
$\theta_j = \theta_j - \alpha \frac{1}{m} \sum_{i=1}^{m} ((h_\theta (x^{(i)})  - y^{(i)}) \cdot  x_j^{(i)})$ , $j$ from $0$ to $n$
	
**}**

虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的$h_\theta(x) $与线性回归中不同，所以实际上是不一样的。另外，在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。


### 4. 其他高级算法

一些梯度下降算法之外的选择： 除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，例如：

+ 共轭梯度**（Conjugate Gradient）**
+ 局部优化法**(Broyden fletcher goldfarb shann,BFGS)**
+ 有限内存局部优化法**(LBFGS) **

这三种算法有许多优点：

一个是使用这其中任何一个算法，你通常不需要手动选择学习率$\alpha$ ，所以对于这些算法的一种思路是，给出计算导数项和代价函数的方法，你可以认为算法有一个智能的内部循环，而且，事实上，他们确实有一个智能的内部循环，称为**线性搜索(line search)**算法，它可以自动尝试不同的学习速率$\alpha$ ，并自动选择一个最佳的学习速率$\alpha$ ，因此它甚至可以为每次迭代选择不同的学习速率，不需要自己选择。这些算法实际上在做更复杂的事情，不仅仅是选择一个好的学习速率，所以往往最终比梯度下降收敛得快多了

### 5. 多类别分类：一对多

事实上, 在分类问题中, 很多情况下并不是标准的二分类, 而是多个分类选项  ，例如现在需要对邮件进行分类, 希望能够区分开来自工作的邮件、来自朋友的邮件、来自家人的邮件或者是有关兴趣爱好的邮件， 这样就变成了一个四分类问题， y的取值可以为1、2、3、4

![](http://www.ai-start.com/ml2014/images/b72863ce7f85cd491e5b940924ef5a5f.png)

对于一个多分类问题我们的数据集分布可能如上图所示, 每种类别的数据用不同的形状表示。

在分类过程中, 我们可以将每种类别的数据单独拿出来观察,  例如在观察class1时,  可以将整个数据集分为 是class1 和 非class1 两种类别, 这样就又回到了二分类的问题上。 同样对其他两个类别也进行同样的操作。

这样在每次进行预测的时候，都需要使用三个分类器, 得到三个预测的结果, 此时选择值最大的类别作为预测输出即可。


## 五、正则化(Regularization)

### 1. 过拟合的问题

到现在为止，我们已经学习了线性回归和逻辑回归，它们能够有效地解决许多问题，但是当将它们应用到某些特定的机器学习应用时，会遇到**过拟合(over-fitting)**的问题，会导致它们效果很差。

下面是一个回归问题的例子

![](http://www.ai-start.com/ml2014/images/72f84165fbf1753cd516e65d5e91c0d3.jpg)

+ 第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集；

+ 第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：**预测新数据**。我们可以看出，若给出一个新的值使之预测，它将表现的很差，这就是过拟合，虽然能非常好地适应我们的训练集，但在新输入变量进行预测时可能会效果不好；

+ 而第二个模型才是我们想要的, 既拟合了数据集，也可以对新变量进行预测。


分类问题中也存在这样的问题：

![](http://www.ai-start.com/ml2014/images/be39b497588499d671942cc15026e4a2.jpg)

从多项式的角度理解， x的次数越高，拟合的越好，但相应的预测的能力就可能变差， 处理过拟合方法有两种思路。

+ 丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如PCA）
正则化。

+ 保留所有的特征，但是减少参数的大小。

### 2. 代价函数

我们可以从之前的例子$ h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2^2 +  \theta_3x_3^3 + \theta_4x_4^4$可以看出，是那些高次项导致了过拟合的产生，所以如果能让这些高次项的系数接近于0的话，就能很好的拟合了。 所以正则化的基本方法就是在一定程度上减小高次项参数对应$\theta$的值。

我们想要减少$\theta_3$和$\theta_4$的大小，就需要修改代价函数, 使得在代价函数求最小值的时候, 能够使$\theta_3$和$\theta_4$尽量小,  修改后的代价函数如下： 

$$ J(\theta) = \frac{1}{2m} [\sum_{i=1}^{m} {(h_\theta (x^{(i)})  - y^{(i)})}^2 + 10000\theta_3^2 + 10000\theta_4^2]$$

这样代价函数所求出来的$\theta_3$和$\theta_4$的影响就会小很多, 但事实上我们并不知道具体应该惩罚哪些$\theta$, 所以我们选择对所有参数都进行惩罚， 但惩罚的程度由代价函数最小化过程计算得出, 这样就得到了一个简单的处理过拟合的方法。

$$ J(\theta) = \frac{1}{2m} [\sum_{i=1}^{m} {(h_\theta (x^{(i)})  - y^{(i)})}^2 + \lambda\sum_{j=1}^{n}{\theta_j^2}]$$

其中$\lambda$又称为**正则化参数（Regularization Parameter）**。*(根据惯例，我们不对$\theta_0$进行惩罚)*。$\lambda$的选择不能过大或者过小。
	
+ 如果过小, 则对过拟合的优化程度不明显

+ 如果过大， 则所有的参数$\theta$都将受到较大的惩罚, 最终模型将近似为$h_\theta(x) = \theta_0$

经过正则化处理的模型与原模型的可能对比如下图所示：

![](http://www.ai-start.com/ml2014/images/ea76cc5394cf298f2414f230bcded0bd.jpg)

### 3. 线性回归正则化

对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程。

**若采用梯度下降法**

正则化线性回归的代价函数为：

$$ J(\theta) = \frac{1}{2m} [\sum_{i=1}^{m} {(h_\theta (x^{(i)})  - y^{(i)})}^2 + \lambda\sum_{j=1}^{n}{\theta_j^2}]$$

对其使用梯度下降算法可以得到: 

**Repeat until convergence {**
		
$\theta_0 = \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^{m} ((h_\theta (x^{(i)})  - y^{(i)}) \cdot  x_0^{(i)})$

$\theta_j = \theta_j - \alpha [ \frac{1}{m} \sum_{i=1}^{m} ((h_\theta (x^{(i)})  - y^{(i)}) \cdot  x_j^{(i)}) + \frac{\lambda}{m}\theta_j]    ,    j = 1, 2 ... n $ 
	
**}**

其中对于$\theta$不为0的情况, 可以简化为如下式子, 相比原来的梯度下降算法, 相当于对$\theta$每次减去了一个额外的值

$$\theta_j = \theta_j(1 - \alpha\frac{\lambda}{m} ) -  \alpha\frac{1}{m} \sum_{i=1}^{m} ((h_\theta (x^{(i)})  - y^{(i)}) \cdot  x_j^{(i)})$$


**若采用正规方程**

我们同样也可以利用正规方程来求解正则化线性回归模型，方法如下所示,  其中矩阵的尺寸为$(n+1)*(n+1)$

![](http://www.ai-start.com/ml2014/images/71d723ddb5863c943fcd4e6951114ee3.png)


### 4. 逻辑回归正则化

对于逻辑回归，我们也给代价函数增加一个正则化的表达式，得到代价函数：

$$ J(\theta) = \frac{1}{m}  \sum_{i=1}^{m} [ -y^{(i)} * log(h_\theta (x^{(i)})) - (1-y^{(i)}) * log( 1- h_\theta (x^{(i)}) ] + \frac{\lambda}{2m}\sum_{j=1}^{n}{\theta_j^2}$$

要最小化该代价函数，通过求导，得出梯度下降算法为：

**Repeat until convergence {**
		
$\theta_0 = \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^{m} ((h_\theta (x^{(i)})  - y^{(i)}) \cdot  x_0^{(i)})$

$\theta_j = \theta_j - \alpha [ \frac{1}{m} \sum_{i=1}^{m} ((h_\theta (x^{(i)})  - y^{(i)}) \cdot  x_j^{(i)}) + \frac{\lambda}{m}\theta_j]    ,    j = 1, 2 ... n $ 
	
**}**


+ 虽然正则化的逻辑回归中的梯度下降和正则化的线性回归中的表达式看起来一样，但由于两者$h_\theta(x)$的不同所以计算时还是有很大差别。

+ $\theta_0$不参与其中的任何一个正则化。


