#吴恩达机器学习


## 一、 监督学习与无监督学习

+ 监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。(关键在于数据集的分类标签或数值是已知的)在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。比如你朋友那个新房子的价格。用术语来讲，这叫做回归问题。我们试着推测出一个连续值的结果，即房子的价格。

	+ 回归问题: 预测连续值的问题, 例如判断房子的价格
	
	+ 分类问题 ： 预测离散值的问题, 例如通过肿瘤大小、病人年龄等不同维度判断肿瘤是否为良性。

+ 无监督学习指数据集本身并无标注信息或分类信息 , 例如给出一组数据需要算法自动根据特征信息进行分类, 即无监督学习中的聚类算法 。

## 二、 线性回归与梯度下降

### 1. 线性回归

![](http://img.fhypayaso.cn/QQ20200820-143637%402x.png)

**单变量的线性回归函数**$h_\theta(x) $ ：

$$ h_\theta(x) = \theta_0 + \theta_1x$$

以房价问题为例,其中

+ $x$ :房子的平米数
+ $h_\theta(x)$ :房子的售价
+ $\theta_1$ :拟合直线的斜率
+ $\theta_0$ : 拟合直线与纵轴焦点

### 2. 代价函数

而对于不同的$\theta_0$和$\theta_1$ 所拟合的直线与真实数据都会存在一定误差, 可以用**代价函数**来表示,


$$ J(\theta_0 , \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} {(h_\theta (x^{(i)})  - y^{(i)})}^2$$

即求回归直线与真实数据集的误差平方和并求平均, 所以目标变成求$minimize J(\theta_0 , \theta_1)$所对应的$\theta_0$和$\theta_1$

对于只有一个参数的回归直线, 其代价函数图像如下所示, 可以看到代价函数为一个最小值为0的二次曲线 

![](http://img.fhypayaso.cn/QQ20200820-150438@2x.png)

而对于两个参数的回归直线, 代价函数为

![](http://img.fhypayaso.cn/QQ20200820-150717@2x.png)
	
### 3. 梯度下降

梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数$J(\theta_0 , \theta_1)$的最小值。

梯度下降背后的思想是：开始时我们随机选择一个$\theta$参数的组合，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合，可能会找到不同的局部最小值

![](http://img.fhypayaso.cn/QQ20200820-151420@2x.png)

例如一个复杂的代价函数的三维表示, 在山顶位置任选一点, 并且每次都向着下降幅度最大的方向前进, 但是最终能够下降到哪个最低点, 与初始位置的选择有关, 批量梯度下降**(batch gradient descent)**的算法公式为

![](http://img.fhypayaso.cn/QQ20200820-152208@2x.png)


其中$\alpha$是学习率**（learning rate）**，它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的偏导数。

其中有一个很关键的步骤在于，所有的参数都需要**同时**进行变化, 而不是先更新$\theta_0$再更新$\theta_1$,  因为这样的话先更新的参数会影响后面参数更新的结果。

![](http://img.fhypayaso.cn/QQ20200820-152659@2x.png)

我们还是采用一个参数的情况来理解梯度下降公式,  对于代价函数图像上的某一个点, 如果当前切线的斜率为正, 即导数为正数，那么在应用公式的时 ,  减去正数即向负向移动, 损失函数下降。而如果斜率为负, 相当于减去负数向正向移动, 损失函数依然下降。

其中学习率$\alpha$的选择不能过大或者过小, 若$\alpha$过小, 会导致每次移动下降幅度过小, 下降速度慢。如果$\alpha$太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果太大，它会导致无法收敛，甚至发散。

同时$\alpha$在下降过程中也不需要额外变化。在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，所以当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度。所以实际上没有必要再另外减小$\alpha$。
	

### 4. 线性回归中的梯度下降

现在我们回顾一下两个算法:

+ 线性回归算法: 

	+ 回归方程 : $ h_\theta(x) = \theta_0 + \theta_1x$
	
	+ 损失函数: $ J(\theta_0 , \theta_1) = \frac{1}{2m} \sum_{i=0}^{m} {(h_\theta (x^{(i)})  - y^{(i)})}^2$

+ 梯度下降算法

	**Repeat until convergence {**
	
	$\theta_j = \theta_j - \alpha \frac{\vartheta}{\vartheta\theta_j} J(\theta_0 , \theta_1) $
	
	$ for( j = 1  ， j = 0) $
	
	**}**
	
将损失函数带入到梯度下降算法 , 即: 

$$\frac{\vartheta}{\vartheta\theta_j} J(\theta_0 , \theta_1)  = \frac{\vartheta}{\vartheta\theta_j} \frac{1}{2m} \sum_{i=1}^{m} {(h_\theta (x^{(i)})  - y^{(i)})}^2$$

+ **j=0**

$$\frac{\vartheta}{\vartheta\theta_0} J(\theta_0 , \theta_1)  = \frac{1}{m} \sum_{i=1}^{m} (h_\theta (x^{(i)})  - y^{(i)})$$

+ **j=1**

$$\frac{\vartheta}{\vartheta\theta_1} J(\theta_0 , \theta_1)  = \frac{1}{m} \sum_{i=1}^{m} ((h_\theta (x^{(i)})  - y^{(i)}) \cdot  x^{(i)})$$

则梯度下降算法可以改写为
	
**Repeat until convergence {**
	
$\theta_0 = \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta (x^{(i)})  - y^{(i)}) $
	
$\theta_1 = \theta_1 - \alpha \frac{1}{m} \sum_{i=1}^{m} ((h_\theta (x^{(i)})  - y^{(i)}) \cdot  x^{(i)})$
	
**}**

该算法有时也称为批量梯度下降，指的是在梯度下降的每一步中，都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都需要对所有m个训练样本求和。因此，批量梯度下降法这个名字说明了我们需要考虑所有这一"批"训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种"批量"型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。


## 三、 多元线性回归






