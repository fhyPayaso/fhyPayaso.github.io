算法面经整理

### 机器学习


#### 基础

+ 机器学习分类

	+ 有监督无监督
	+ 生成/判别

+ LR (逻辑回归)相关
	+ 手推整个流程 
	+ 损失函数解释
	+ 交叉熵损失为什么有log项？
	+ 常规LR：Loss函数，激活函数，为什么不用最小二乘


+ 正则方法
	+ 常见正则化方法 有啥不一样的
	+ 梯度学习优化算法有哪些
	+ L1正则与L2正则	
	+ 带动量不带动量

+ SVM 支持向量机

	+ 手推

	
#### 树模型

+ 决策树

	+ ID3, C4.5 , CART

+ 模型融合

	+ boosting 和 bagging的区别

	+  随机森林(RF)

	+ AdaBoosting

	+ GB

	+ GBDT
		+ 残差和梯度之间的关系
		+ 手推GBDT公式（要体现找分裂点的过程）
		+ gbdt的梯度为什么能代替残差（我刚说了用损失函数的一阶泰勒还没说完就问了下一个）
		+ gbdt的损失函数是什么（我说可以很多啊最小二乘啥的，他好像不是想问这个）
		+ gbdt如果有100棵树，每棵树的输出是什么（我觉得是拟合的上一步输出与目标值的残差，但是具体公式不清楚）

	+ XGB

	+ LigntGBM

		+ lightgbm创建每棵树时速度是均匀的吗？（不会）
		+ lightgbm训练时和特征数目更相关还是样本数目更相关（不会）
		+ 连续特征值在lightgbm中如何找到分界点
		+ lightgbm该如何调参：找最佳组合-怎么找的-暴力找的-网格搜索有没有用过？-⑤
		+ lightgbm如果过拟合了，首先调哪几个参数？（我说的l1和l2...不知道对不对....）

	+ 模型之间比较

		+ boosting中每一轮loss是怎么样计算的？
		+ XGB和LigntGBM如何对GBDT进行优化的  
		+ lightGBM，XGBoost，GBDT怎么分裂 怎么着最优找分裂点（xgboost如何选择最优分割点）



#### 推荐相关

+ 介绍一下推荐系统的整体流程

+ CTR : FFM和FM

+ 介绍和手写FM

+ 介绍和手写deepfm

+ itemCF和userCF的区别

+ 协同过滤 itemcf 怎么改进

+ Word2vec 调参

+ Learning to Rank：Point-wise、Pair-wise 和 List-wise区别

+ 深度学习推荐系统 ncf DIN和DIEN网络结构介绍


#### 比赛/项目相关

+ 特征工程

+ 缺失值如何处理

+ 交叉验证

+ 学习曲线

+ ROC(查准率，查全率) ，AUC

	+ 常见的评估指标有哪些
	+ 为什么AUC面积大证明性能强	
	+ 手撕AUC代码计算
	+ AUC有啥缺陷
	+ 精确率、准确率、召回率

#### 无监督

+ 聚类算法Kmeans

	+ 时间复杂度






#### 其他

+ w2v
+ 决策树怎么选择特征。信息增益和信息增益比的区别
+ 逻辑回归介绍
+ svm介绍，以及松弛变量的理解

+ 贝叶斯公式应用题
+ 朴素贝叶斯思想
+ 手推lr梯度, 交叉熵损失为什么有log项？
+ 常见优化器
+ 生成模型与判别模型区别
+ 大规模LR参数稀疏解怎么求
+ 最小二乘法在什么条件下与极大似然估计等价？


+ 大规模深度CTR预估中Embedding怎么在线更新
+ 大规模深度CTR预估中Adam与Adagrad怎么选择，为什么对稀疏离散特征使用Adagrad, 而对连续特征(Dense)使用Adam？


+ 决策树：GBDT、xgb、LigntGBM区别以及运用场景
+ 基尼系数是什么？为什么用基尼系数不用熵？（因为熵log运算耗时）
+ LR与SVM区别

+ bagging/boosting区别
+ 逻辑回归和决策树区别


### 深度学习

+ embedding ？

+ BN : batch normalization的过程和优缺点

+ Dropout

+ LSTM

+ 梯度消失和梯度爆炸： 梯度消失梯度爆炸解决办法

+ unet

+ 3.训练时某些参数收敛了，某些没有，怎么办，我不知道，后来他说这个是自适应的相关的，这个我没了解过。

+ focal loss介绍








	




